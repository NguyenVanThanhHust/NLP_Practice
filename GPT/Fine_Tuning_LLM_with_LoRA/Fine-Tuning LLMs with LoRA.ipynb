{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96112c9f",
   "metadata": {},
   "source": [
    "https://ai.plainenglish.io/creating-your-own-chatgpt-a-guide-to-fine-tuning-llms-with-lora-d7817b77fac0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f082c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, isdir, isfile\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "HC3_DATA_FOLDER = \"../../../input/HC3\"\n",
    "ChatGPT_promts_DATA_FOLDER = \"../../../input/ChatGPT_promts/\"\n",
    "hc3_json_all = join(HC3_DATA_FOLDER, \"all.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbaba29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "019b4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hc3_json_all, \"r\") as handle:\n",
    "    hc3_list = list(handle)\n",
    "\n",
    "questions, answers = [], []\n",
    "    \n",
    "handle = open(hc3_json_all)\n",
    "lines = handle.readlines()\n",
    "for line in lines:\n",
    "    row = json.loads(line)\n",
    "    for answer in row[\"human_answers\"]:\n",
    "        questions.append(\"Human: \"+row[\"question\"])\n",
    "        answers.append(\"Assistant: \"+answer)\n",
    "    for answer in row[\"chatgpt_answers\"]:\n",
    "        questions.append(\"Human: \"+row[\"question\"])\n",
    "        answers.append(\"Assistant: \"+answer)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"question\"] = questions\n",
    "df[\"answer\"] = answers\n",
    "\n",
    "df.to_csv(join(\"data\", \"train.csv\"), index=False)\n",
    "\n",
    "chatgpt_promts = pd.read_csv(join(ChatGPT_promts_DATA_FOLDER, \"train.csv\"))\n",
    "chatgpt_promts.to_csv(join(\"data\", \"val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd7d5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup, DataCollatorForSeq2Seq\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, get_peft_model_state_dict\n",
    "from peft.utils.other import fsdp_auto_wrap_policy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daa1f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    accelerator = Accelerator()\n",
    "    model_name_or_path = \"google/flan-t5-xxl\"\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    lr = 1e-4\n",
    "    num_epochs = 1\n",
    "    train_data = \"./data/train.csv\"\n",
    "    test_data = \"./data/val.csv\"\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "    checkpoint_name = \"chaT5_lora.pt\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    accelerator.print(model.print_trainable_parameters())\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "            'csv', data_files={\n",
    "                \"train\": train_data,\n",
    "                \"validation\": test_data,\n",
    "            }, \n",
    "            cache_dir=\"./cache\")\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [doc for doc in examples[\"question\"]]\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, max_length=max_length, padding=True, truncation=True)\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples[\"answer\"], max_length=max_length, padding=True, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=16,\n",
    "            remove_columns=dataset[\"train\"].column_names,\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer, model=model)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, collate_fn=data_collator, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\n",
    "        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n",
    "\n",
    "    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n",
    "    )\n",
    "    accelerator.print(model)\n",
    "    accelerator.state.deepspeed_plugin.zero_stage == 3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step%1000 == 0:\n",
    "                print(\"loss: \",loss.detach().float())\n",
    "                accelerator.wait_for_everyone()\n",
    "                if accelerator.is_main_process:\n",
    "                    accelerator.save(\n",
    "                        get_peft_model_state_dict(model, state_dict=accelerator.get_state_dict(model)), checkpoint_name\n",
    "                    )\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n",
    "            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "        eval_epoch_loss = eval_loss / len(train_dataloader)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(eval_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        accelerator.save(\n",
    "            get_peft_model_state_dict(model, state_dict=accelerator.get_state_dict(model)), checkpoint_name\n",
    "        )\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae4543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9429df66af1d475a8feb0acf882879e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e9f79ebb8d42539212e05c70cdd588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dfa9dc12f649e8a130ae44d4a7fcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34383e611f345b5b5a39ea0f92598eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00005.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba585a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb3ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
